{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation with Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "from src.rag_system import RAGSystem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç—ä–ø–∫–∞ 2: –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–π RAG —Å–∏—Å—Ç–µ–º–∞—Ç–∞\n",
    "print(\"Initializing RAG System...\\n\")\n",
    "\n",
    "rag = RAGSystem(\n",
    "    vector_db_path=\"vector_db\",\n",
    "    model_name=\"llama3.2:3b\"  # –ü—Ä–æ–º–µ–Ω–∏ –∞–∫–æ –∏–∑–ø–æ–ª–∑–≤–∞—à –¥—Ä—É–≥ –º–æ–¥–µ–ª\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì RAG System ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç—ä–ø–∫–∞ 3: –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ Test Dataset\n",
    "\n",
    "–©–µ —Å—ä–∑–¥–∞–¥–µ–º —Ç–µ—Å—Ç –≤—ä–ø—Ä–æ—Å–∏ —Å –∏–∑–≤–µ—Å—Ç–Ω–∏ –ø—Ä–∞–≤–∏–ª–Ω–∏ –æ—Ç–≥–æ–≤–æ—Ä–∏ (ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases: –í—Å–µ–∫–∏ –∏–º–∞ query –∏ –∫–æ–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏ —Ç—Ä—è–±–≤–∞ –¥–∞ –±—ä–¥–∞—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏\n",
    "# relevant_doc_ids —Å—ä–¥—ä—Ä–∂–∞ product + category –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        'id': 'test_001',\n",
    "        'query': 'How do I install CloudSync Pro on Windows?',\n",
    "        'relevant_products': ['CloudSync Pro'],\n",
    "        'relevant_categories': ['installation'],\n",
    "        'expected_answer_contains': ['download', 'install', 'windows']\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_002',\n",
    "        'query': 'My files are not syncing in CloudSync Pro',\n",
    "        'relevant_products': ['CloudSync Pro'],\n",
    "        'relevant_categories': ['troubleshooting'],\n",
    "        'expected_answer_contains': ['sync', 'internet', 'connection']\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_003',\n",
    "        'query': 'What is selective sync feature?',\n",
    "        'relevant_products': ['CloudSync Pro'],\n",
    "        'relevant_categories': ['features'],\n",
    "        'expected_answer_contains': ['selective', 'sync', 'folders']\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_004',\n",
    "        'query': 'How to backup files with DataVault?',\n",
    "        'relevant_products': ['DataVault'],\n",
    "        'relevant_categories': ['features', 'installation'],\n",
    "        'expected_answer_contains': ['backup', 'files']\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_005',\n",
    "        'query': 'DataVault backup is failing',\n",
    "        'relevant_products': ['DataVault'],\n",
    "        'relevant_categories': ['troubleshooting'],\n",
    "        'expected_answer_contains': ['backup', 'disk space', 'destination']\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_006',\n",
    "        'query': 'TeamChat video call quality is bad',\n",
    "        'relevant_products': ['TeamChat'],\n",
    "        'relevant_categories': ['troubleshooting'],\n",
    "        'expected_answer_contains': ['video', 'internet', 'connection']\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_007',\n",
    "        'query': 'How to send messages in TeamChat?',\n",
    "        'relevant_products': ['TeamChat'],\n",
    "        'relevant_categories': ['features'],\n",
    "        'expected_answer_contains': ['message', 'channel']\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_008',\n",
    "        'query': 'Cannot create new project in ProjectHub',\n",
    "        'relevant_products': ['ProjectHub'],\n",
    "        'relevant_categories': ['troubleshooting'],\n",
    "        'expected_answer_contains': ['project', 'limit', 'permission']\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_009',\n",
    "        'query': 'What is Kanban board in ProjectHub?',\n",
    "        'relevant_products': ['ProjectHub'],\n",
    "        'relevant_categories': ['features'],\n",
    "        'expected_answer_contains': ['kanban', 'board', 'task']\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_010',\n",
    "        'query': 'How much storage do I get with CloudSync Pro?',\n",
    "        'relevant_products': ['CloudSync Pro'],\n",
    "        'relevant_categories': ['faq'],\n",
    "        'expected_answer_contains': ['storage', 'gb', 'pro']\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_011',\n",
    "        'query': 'Is TeamChat available on mobile?',\n",
    "        'relevant_products': ['TeamChat'],\n",
    "        'relevant_categories': ['faq'],\n",
    "        'expected_answer_contains': ['mobile', 'ios', 'android']\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_012',\n",
    "        'query': 'How secure are DataVault backups?',\n",
    "        'relevant_products': ['DataVault'],\n",
    "        'relevant_categories': ['faq'],\n",
    "        'expected_answer_contains': ['secure', 'encrypt', 'aes']\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"‚úì Created {len(test_cases)} test cases\")\n",
    "print(\"\\nSample test case:\")\n",
    "print(f\"Query: {test_cases[0]['query']}\")\n",
    "print(f\"Expected product: {test_cases[0]['relevant_products']}\")\n",
    "print(f\"Expected category: {test_cases[0]['relevant_categories']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç—ä–ø–∫–∞ 4: Evaluation –§—É–Ω–∫—Ü–∏–∏\n",
    "\n",
    "–§—É–Ω–∫—Ü–∏–∏ –∑–∞ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant_result(retrieved_metadata: Dict, test_case: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ–≤–µ—Ä—è–≤–∞ –¥–∞–ª–∏ retrieved –¥–æ–∫—É–º–µ–Ω—Ç –µ relevant –∑–∞ test case\n",
    "    \n",
    "    Args:\n",
    "        retrieved_metadata: Metadata –Ω–∞ retrieved –¥–æ–∫—É–º–µ–Ω—Ç\n",
    "        test_case: Test case —Å relevant_products –∏ relevant_categories\n",
    "    \n",
    "    Returns:\n",
    "        True –∞–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç—ä—Ç –µ relevant\n",
    "    \"\"\"\n",
    "    product_match = retrieved_metadata['product'] in test_case['relevant_products']\n",
    "    category_match = retrieved_metadata['category'] in test_case['relevant_categories']\n",
    "    \n",
    "    return product_match and category_match\n",
    "\n",
    "\n",
    "def calculate_precision_at_k(retrieved_metadatas: List[Dict], test_case: Dict, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Precision@K: –ö–∞–∫—ä–≤ –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç top K –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ç–µ —Å–∞ relevant?\n",
    "    \n",
    "    Formula: (# relevant docs in top K) / K\n",
    "    \"\"\"\n",
    "    top_k = retrieved_metadatas[:k]\n",
    "    relevant_count = sum(1 for meta in top_k if is_relevant_result(meta, test_case))\n",
    "    return relevant_count / k if k > 0 else 0\n",
    "\n",
    "\n",
    "def calculate_recall_at_k(retrieved_metadatas: List[Dict], test_case: Dict, k: int, total_relevant: int) -> float:\n",
    "    \"\"\"\n",
    "    Recall@K: –ö–∞–∫—ä–≤ –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –≤—Å–∏—á–∫–∏ relevant –¥–æ–∫—É–º–µ–Ω—Ç–∏ –Ω–∞–º–µ—Ä–∏—Ö–º–µ?\n",
    "    \n",
    "    Formula: (# relevant docs retrieved) / (total # relevant docs)\n",
    "    \"\"\"\n",
    "    top_k = retrieved_metadatas[:k]\n",
    "    found_count = sum(1 for meta in top_k if is_relevant_result(meta, test_case))\n",
    "    return found_count / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "\n",
    "def calculate_hit_rate(retrieved_metadatas: List[Dict], test_case: Dict, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Hit Rate: –ù–∞–º–µ—Ä–∏—Ö–º–µ –ª–∏ –ø–æ–Ω–µ –µ–¥–∏–Ω relevant –¥–æ–∫—É–º–µ–Ω—Ç?\n",
    "    \n",
    "    Returns 1.0 if yes, 0.0 if no\n",
    "    \"\"\"\n",
    "    top_k = retrieved_metadatas[:k]\n",
    "    for meta in top_k:\n",
    "        if is_relevant_result(meta, test_case):\n",
    "            return 1.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calculate_mrr(retrieved_metadatas: List[Dict], test_case: Dict) -> float:\n",
    "    \"\"\"\n",
    "    MRR (Mean Reciprocal Rank): –ù–∞ –∫–æ—è –ø–æ–∑–∏—Ü–∏—è –µ –ø—ä—Ä–≤–∏—è—Ç relevant –¥–æ–∫—É–º–µ–Ω—Ç?\n",
    "    \n",
    "    Returns 1/rank (so higher rank = better score)\n",
    "    Example: First relevant at position 2 ‚Üí MRR = 1/2 = 0.5\n",
    "    \"\"\"\n",
    "    for rank, meta in enumerate(retrieved_metadatas, start=1):\n",
    "        if is_relevant_result(meta, test_case):\n",
    "            return 1.0 / rank\n",
    "    return 0.0  # No relevant doc found\n",
    "\n",
    "\n",
    "print(\"‚úì Evaluation functions created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç—ä–ø–∫–∞ 5: Run Evaluation\n",
    "\n",
    "**–¢–æ–≤–∞ –º–æ–∂–µ –¥–∞ –æ—Ç–Ω–µ–º–µ 2-3 –º–∏–Ω—É—Ç–∏** –∑–∞—â–æ—Ç–æ —Ç—Ä—è–±–≤–∞ –¥–∞ –ø—É—Å–Ω–µ–º –≤—Å–∏—á–∫–∏ test queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞ evaluation\n",
    "K_VALUES = [1, 3, 5]  # –©–µ —Ç–µ—Å—Ç–≤–∞–º–µ —Å —Ä–∞–∑–ª–∏—á–Ω–∏ K\n",
    "TOTAL_RELEVANT_DOCS = 5  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–º–µ ~5 relevant docs per query\n",
    "\n",
    "# –°—ä—Ö—Ä–∞–Ω—è–≤–∞–π —Ä–µ–∑—É–ª—Ç–∞—Ç–∏\n",
    "evaluation_results = []\n",
    "\n",
    "print(\"Running evaluation...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for test_case in tqdm(test_cases, desc=\"Evaluating test cases\"):\n",
    "    query = test_case['query']\n",
    "    \n",
    "    # Retrieve –¥–æ–∫—É–º–µ–Ω—Ç–∏ (–≤–∑–∏–º–∞–º–µ max K –∑–∞ –¥–∞ –º–æ–∂–µ–º –¥–∞ —Ç–µ—Å—Ç–≤–∞–º–µ –≤—Å–∏—á–∫–∏ K —Å—Ç–æ–π–Ω–æ—Å—Ç–∏)\n",
    "    search_results = rag.search(query, n_results=max(K_VALUES))\n",
    "    retrieved_metadatas = search_results['metadatas']\n",
    "    distances = search_results['distances']\n",
    "    \n",
    "    # –ò–∑—á–∏—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ –≤—Å—è–∫–æ K\n",
    "    result = {\n",
    "        'test_id': test_case['id'],\n",
    "        'query': query,\n",
    "        'relevant_products': test_case['relevant_products'],\n",
    "        'relevant_categories': test_case['relevant_categories'],\n",
    "    }\n",
    "    \n",
    "    for k in K_VALUES:\n",
    "        precision = calculate_precision_at_k(retrieved_metadatas, test_case, k)\n",
    "        recall = calculate_recall_at_k(retrieved_metadatas, test_case, k, TOTAL_RELEVANT_DOCS)\n",
    "        hit_rate = calculate_hit_rate(retrieved_metadatas, test_case, k)\n",
    "        \n",
    "        result[f'precision@{k}'] = precision\n",
    "        result[f'recall@{k}'] = recall\n",
    "        result[f'hit_rate@{k}'] = hit_rate\n",
    "    \n",
    "    # MRR (–Ω–µ –∑–∞–≤–∏—Å–∏ –æ—Ç K)\n",
    "    result['mrr'] = calculate_mrr(retrieved_metadatas, test_case)\n",
    "    \n",
    "    # –ó–∞–ø–∞–∑–∏ retrieved results –∑–∞ debugging\n",
    "    result['top_3_results'] = [\n",
    "        {\n",
    "            'product': meta['product'],\n",
    "            'category': meta['category'],\n",
    "            'title': meta['title'],\n",
    "            'distance': dist\n",
    "        }\n",
    "        for meta, dist in zip(retrieved_metadatas[:3], distances[:3])\n",
    "    ]\n",
    "    \n",
    "    evaluation_results.append(result)\n",
    "\n",
    "print(\"\\n‚úì Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç—ä–ø–∫–∞ 6: –ü—Ä–µ–≥–ª–µ–¥ –Ω–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª–Ω–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–∫–∞–∂–∏ –ø—ä—Ä–≤–∏—Ç–µ 3 —Ç–µ—Å—Ç–∞ –¥–µ—Ç–∞–π–ª–Ω–æ\n",
    "print(\"Sample Evaluation Results:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(evaluation_results[:3], 1):\n",
    "    print(f\"\\nTest {i}: {result['test_id']}\")\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Expected: {result['relevant_products']} - {result['relevant_categories']}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Precision@3: {result['precision@3']:.2f}\")\n",
    "    print(f\"  Recall@3: {result['recall@3']:.2f}\")\n",
    "    print(f\"  Hit Rate@3: {result['hit_rate@3']:.2f}\")\n",
    "    print(f\"  MRR: {result['mrr']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nTop 3 Retrieved:\")\n",
    "    for j, doc in enumerate(result['top_3_results'], 1):\n",
    "        relevant = \"‚úì\" if (doc['product'] in result['relevant_products'] and \n",
    "                          doc['category'] in result['relevant_categories']) else \"‚úó\"\n",
    "        print(f\"  {j}. {relevant} {doc['product']} - {doc['category']} (dist: {doc['distance']:.4f})\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç—ä–ø–∫–∞ 7: Aggregate Metrics\n",
    "\n",
    "–ò–∑—á–∏—Å–ª—è–≤–∞–º–µ —Å—Ä–µ–¥–Ω–∏—Ç–µ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ –∑–∞ –≤—Å–∏—á–∫–∏ –º–µ—Ç—Ä–∏–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑—á–∏—Å–ª–∏ —Å—Ä–µ–¥–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏\n",
    "def calculate_average_metrics(results: List[Dict]) -> Dict:\n",
    "    \"\"\"–ò–∑—á–∏—Å–ª–∏ —Å—Ä–µ–¥–Ω–∏—Ç–µ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ –Ω–∞ –≤—Å–∏—á–∫–∏ –º–µ—Ç—Ä–∏–∫–∏\"\"\"\n",
    "    avg_metrics = {}\n",
    "    \n",
    "    # –ó–∞ –≤—Å—è–∫–æ K\n",
    "    for k in K_VALUES:\n",
    "        avg_metrics[f'precision@{k}'] = np.mean([r[f'precision@{k}'] for r in results])\n",
    "        avg_metrics[f'recall@{k}'] = np.mean([r[f'recall@{k}'] for r in results])\n",
    "        avg_metrics[f'hit_rate@{k}'] = np.mean([r[f'hit_rate@{k}'] for r in results])\n",
    "    \n",
    "    # MRR\n",
    "    avg_metrics['mrr'] = np.mean([r['mrr'] for r in results])\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "avg_metrics = calculate_average_metrics(evaluation_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL EVALUATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "for k in K_VALUES:\n",
    "    print(f\"K = {k}:\")\n",
    "    print(f\"  Precision@{k}: {avg_metrics[f'precision@{k}']:.3f}\")\n",
    "    print(f\"  Recall@{k}:    {avg_metrics[f'recall@{k}']:.3f}\")\n",
    "    print(f\"  Hit Rate@{k}:  {avg_metrics[f'hit_rate@{k}']:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"MRR (Mean Reciprocal Rank): {avg_metrics['mrr']:.3f}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç—ä–ø–∫–∞ 8: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏—Ç–µ\n",
    "\n",
    "–ì—Ä–∞—Ñ–∏–∫–∏ –∑–∞ –ø–æ-–¥–æ–±—Ä–æ —Ä–∞–∑–±–∏—Ä–∞–Ω–µ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ì—Ä–∞—Ñ–∏–∫ 1: –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ K —Å—Ç–æ–π–Ω–æ—Å—Ç–∏\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Precision@K\n",
    "precision_values = [avg_metrics[f'precision@{k}'] for k in K_VALUES]\n",
    "axes[0, 0].plot(K_VALUES, precision_values, marker='o', linewidth=2, markersize=8, color='#3498db')\n",
    "axes[0, 0].set_xlabel('K')\n",
    "axes[0, 0].set_ylabel('Precision')\n",
    "axes[0, 0].set_title('Precision@K')\n",
    "axes[0, 0].set_ylim(0, 1.1)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(K_VALUES)\n",
    "\n",
    "# Recall@K\n",
    "recall_values = [avg_metrics[f'recall@{k}'] for k in K_VALUES]\n",
    "axes[0, 1].plot(K_VALUES, recall_values, marker='o', linewidth=2, markersize=8, color='#2ecc71')\n",
    "axes[0, 1].set_xlabel('K')\n",
    "axes[0, 1].set_ylabel('Recall')\n",
    "axes[0, 1].set_title('Recall@K')\n",
    "axes[0, 1].set_ylim(0, 1.1)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(K_VALUES)\n",
    "\n",
    "# Hit Rate@K\n",
    "hit_rate_values = [avg_metrics[f'hit_rate@{k}'] for k in K_VALUES]\n",
    "axes[1, 0].plot(K_VALUES, hit_rate_values, marker='o', linewidth=2, markersize=8, color='#f39c12')\n",
    "axes[1, 0].set_xlabel('K')\n",
    "axes[1, 0].set_ylabel('Hit Rate')\n",
    "axes[1, 0].set_title('Hit Rate@K')\n",
    "axes[1, 0].set_ylim(0, 1.1)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(K_VALUES)\n",
    "\n",
    "# Comparison bar chart for K=3\n",
    "metrics_k3 = ['Precision@3', 'Recall@3', 'Hit Rate@3']\n",
    "values_k3 = [\n",
    "    avg_metrics['precision@3'],\n",
    "    avg_metrics['recall@3'],\n",
    "    avg_metrics['hit_rate@3']\n",
    "]\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12']\n",
    "bars = axes[1, 1].bar(metrics_k3, values_k3, color=colors)\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Metrics Comparison (K=3)')\n",
    "axes[1, 1].set_ylim(0, 1.1)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values_k3):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{value:.3f}',\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/processed/evaluation_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved to data/processed/evaluation_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç—ä–ø–∫–∞ 9: –î–µ—Ç–∞–π–ª–µ–Ω –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –ø–æ –ø—Ä–æ–¥—É–∫—Ç\n",
    "product_metrics = {}\n",
    "\n",
    "for product in ['CloudSync Pro', 'DataVault', 'TeamChat', 'ProjectHub']:\n",
    "    product_results = [\n",
    "        r for r in evaluation_results \n",
    "        if product in r['relevant_products']\n",
    "    ]\n",
    "    \n",
    "    if product_results:\n",
    "        product_metrics[product] = calculate_average_metrics(product_results)\n",
    "\n",
    "# –ü–æ–∫–∞–∂–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏\n",
    "print(\"\\nPerformance by Product (K=3):\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for product, metrics in product_metrics.items():\n",
    "    print(f\"\\n{product}:\")\n",
    "    print(f\"  Precision@3: {metrics['precision@3']:.3f}\")\n",
    "    print(f\"  Recall@3:    {metrics['recall@3']:.3f}\")\n",
    "    print(f\"  Hit Rate@3:  {metrics['hit_rate@3']:.3f}\")\n",
    "    print(f\"  MRR:         {metrics['mrr']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç—ä–ø–∫–∞ 10: –ó–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—ä–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∞–∫–æ –Ω–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# –ó–∞–ø–∞–∑–∏ –¥–µ—Ç–∞–π–ª–Ω–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏\n",
    "with open('data/processed/evaluation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(evaluation_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úì Detailed results saved to data/processed/evaluation_results.json\")\n",
    "\n",
    "# –ó–∞–ø–∞–∑–∏ –∞–≥—Ä–µ–≥–∏—Ä–∞–Ω–∏ –º–µ—Ç—Ä–∏–∫–∏\n",
    "summary = {\n",
    "    'overall_metrics': avg_metrics,\n",
    "    'product_metrics': product_metrics,\n",
    "    'total_test_cases': len(test_cases),\n",
    "    'k_values': K_VALUES\n",
    "}\n",
    "\n",
    "with open('data/processed/evaluation_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úì Summary saved to data/processed/evaluation_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç—ä–ø–∫–∞ 11: –°—ä–∑–¥–∞–π evaluation —Ñ—É–Ω–∫—Ü–∏—è –∑–∞ –ø–æ-–∫—ä—Å–Ω–∞ —É–ø–æ—Ç—Ä–µ–±–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–ø–∞–∑–∏ evaluation functions –∫–∞—Ç–æ Python —Ñ–∞–π–ª\n",
    "evaluator_code = '''\"\"\"RAG Evaluation Functions\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "def is_relevant_result(retrieved_metadata: Dict, test_case: Dict) -> bool:\n",
    "    \"\"\"Check if retrieved document is relevant\"\"\"\n",
    "    product_match = retrieved_metadata['product'] in test_case['relevant_products']\n",
    "    category_match = retrieved_metadata['category'] in test_case['relevant_categories']\n",
    "    return product_match and category_match\n",
    "\n",
    "def calculate_precision_at_k(retrieved_metadatas: List[Dict], test_case: Dict, k: int) -> float:\n",
    "    \"\"\"Precision@K: (# relevant in top K) / K\"\"\"\n",
    "    top_k = retrieved_metadatas[:k]\n",
    "    relevant_count = sum(1 for meta in top_k if is_relevant_result(meta, test_case))\n",
    "    return relevant_count / k if k > 0 else 0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_metadatas: List[Dict], test_case: Dict, k: int, total_relevant: int) -> float:\n",
    "    \"\"\"Recall@K: (# relevant retrieved) / (total relevant)\"\"\"\n",
    "    top_k = retrieved_metadatas[:k]\n",
    "    found_count = sum(1 for meta in top_k if is_relevant_result(meta, test_case))\n",
    "    return found_count / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "def calculate_hit_rate(retrieved_metadatas: List[Dict], test_case: Dict, k: int) -> float:\n",
    "    \"\"\"Hit Rate: Found at least one relevant doc?\"\"\"\n",
    "    top_k = retrieved_metadatas[:k]\n",
    "    for meta in top_k:\n",
    "        if is_relevant_result(meta, test_case):\n",
    "            return 1.0\n",
    "    return 0.0\n",
    "\n",
    "def calculate_mrr(retrieved_metadatas: List[Dict], test_case: Dict) -> float:\n",
    "    \"\"\"MRR: 1 / (rank of first relevant doc)\"\"\"\n",
    "    for rank, meta in enumerate(retrieved_metadatas, start=1):\n",
    "        if is_relevant_result(meta, test_case):\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_rag_system(rag_system, test_cases: List[Dict], k_values: List[int] = [1, 3, 5]) -> Dict:\n",
    "    \"\"\"Run full evaluation\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        query = test_case['query']\n",
    "        search_results = rag_system.search(query, n_results=max(k_values))\n",
    "        retrieved_metadatas = search_results['metadatas']\n",
    "        \n",
    "        result = {\n",
    "            'test_id': test_case['id'],\n",
    "            'query': query,\n",
    "        }\n",
    "        \n",
    "        for k in k_values:\n",
    "            result[f'precision@{k}'] = calculate_precision_at_k(retrieved_metadatas, test_case, k)\n",
    "            result[f'recall@{k}'] = calculate_recall_at_k(retrieved_metadatas, test_case, k, 5)\n",
    "            result[f'hit_rate@{k}'] = calculate_hit_rate(retrieved_metadatas, test_case, k)\n",
    "        \n",
    "        result['mrr'] = calculate_mrr(retrieved_metadatas, test_case)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_metrics = {}\n",
    "    for k in k_values:\n",
    "        avg_metrics[f'precision@{k}'] = np.mean([r[f'precision@{k}'] for r in results])\n",
    "        avg_metrics[f'recall@{k}'] = np.mean([r[f'recall@{k}'] for r in results])\n",
    "        avg_metrics[f'hit_rate@{k}'] = np.mean([r[f'hit_rate@{k}'] for r in results])\n",
    "    avg_metrics['mrr'] = np.mean([r['mrr'] for r in results])\n",
    "    \n",
    "    return {\n",
    "        'detailed_results': results,\n",
    "        'average_metrics': avg_metrics\n",
    "    }\n",
    "'''\n",
    "\n",
    "with open('src/evaluator.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(evaluator_code)\n",
    "\n",
    "print(\"‚úì Evaluator functions saved to src/evaluator.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "**–ö–∞–∫–≤–æ –Ω–∞–ø—Ä–∞–≤–∏—Ö–º–µ:**\n",
    "- ‚úì –°—ä–∑–¥–∞–¥–æ—Ö–º–µ 12 test cases —Å ground truth\n",
    "- ‚úì –ò–∑—á–∏—Å–ª–∏—Ö–º–µ 4 –∫–ª—é—á–æ–≤–∏ –º–µ—Ç—Ä–∏–∫–∏:\n",
    "  - Precision@K\n",
    "  - Recall@K\n",
    "  - Hit Rate@K\n",
    "  - MRR\n",
    "- ‚úì –¢–µ—Å—Ç–≤–∞—Ö–º–µ —Å K=1, 3, 5\n",
    "- ‚úì –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–∞—Ö–º–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ\n",
    "- ‚úì –ê–Ω–∞–ª–∏–∑–∏—Ä–∞—Ö–º–µ –ø–æ –ø—Ä–æ–¥—É–∫—Ç\n",
    "- ‚úì –ó–∞–ø–∞–∑–∏—Ö–º–µ evaluation functions –∑–∞ re-use\n",
    "\n",
    "**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –Ω–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ:**\n",
    "- **Precision**: –ö–æ–ª–∫–æ —Ç–æ—á–Ω–∏ —Å–∞ –Ω–∞—à–∏—Ç–µ retrieval —Ä–µ–∑—É–ª—Ç–∞—Ç–∏\n",
    "- **Recall**: –ö–æ–ª–∫–æ comprehensive –µ coverage-–∞\n",
    "- **Hit Rate**: –ú–∏–Ω–∏–º–∞–ª–Ω–∞—Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç –Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∞\n",
    "- **MRR**: –ö–æ–ª–∫–æ –¥–æ–±—Ä–µ ranking-–∞ –Ω–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ\n",
    "\n",
    "**–°–ª–µ–¥–≤–∞—â–∞ —Å—Ç—ä–ø–∫–∞:**\n",
    "–û—Ç–≤–æ—Ä–∏ `04_gradio_dashboard.ipynb` –∑–∞ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–µ–Ω UI! üé®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
